{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade firebase-admin\n",
    "%pip install pandas\n",
    "%pip install scipy\n",
    "%pip install numpy\n",
    "%pip install seaborn\n",
    "%pip install matplotlib\n",
    "%pip install statsmodels\n",
    "%pip install scikit-learn\n",
    "%pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Note:*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is optional. The developers of this notebook series have already provided a sample of the haptic touch dataset used for data analysis and machine learning stages of the thesis, *Integrating Haptic Touch on Android Devices for Emotion Recognition in an Emotionally Stimulated Environment*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Data Collection From Firebase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to get the data from the GetEmotion application and convert it into an organized and readable Dataframe. We will be exporting this dataset at the end of this notebook to use for the Data Preprocessing stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for printing the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_df(df):\n",
    "    # Print the DataFrame\n",
    "    with pd.option_context('display.max_rows', 20, 'display.max_columns', None): \n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firebase Firestore Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to have an access to our Firebase firestore database. To do this, we need a JSON file called **Firebase Admin SDK** key that is downloaded from your project in Firebase. Follow these instructions to download the said file:\n",
    "\n",
    "1. Go to your project in [Firebase](https://console.firebase.google.com/).\n",
    "2. Go to **Project Settings**\n",
    "3. Click **Service Accounts**\n",
    "4. Under **Firebase Admin SDK**, make sure that the you have selected *Python* as the *Admin SDK configuration snippet*.\n",
    "5. Click **Generate new private key**.\n",
    "6. Move the private key to this folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the filename of the downloaded private key to `key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `credentials.Certificate()` and pass `key` as parameter to generate a certificate. Store its result to `cred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cred = credentials.Certificate(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then call `firebase_admin.initialize_app()` and pass `cred` as its parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firebase_admin.initialize_app(cred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now call `firestore.client()` and store it in a variable called `db`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = firestore.client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataframe out of the Firebase Firestore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, get the collection of interactions by calling `db.collection('Interactions')` and storing its result to `collection_ref`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_ref = db.collection('Interactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then collect the documents by calling `collection_ref.get()`. Store them to `docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = collection_ref.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize an empty list called `data_list` to store document data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features *acceleration*, *velocity*, and *coordinates* all have nested structures, and should be divided into atomic key-value pairs because their values are crucial to our data analysis and machine learning stages. \n",
    "\n",
    "To do this, we need to flatten them into multiple columns. **Run the code below** to create multiple columns of key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    data = doc.to_dict()\n",
    "\n",
    "    #Flatten acceleration data\n",
    "    acceleration_data = data.get('acceleration')\n",
    "    if isinstance(acceleration_data, dict):\n",
    "        for key, value in acceleration_data.items():\n",
    "            data[f'acceleration_{key}_min'] = value.get('min')\n",
    "            data[f'acceleration_{key}_mean'] = value.get('mean')\n",
    "            data[f'acceleration_{key}_max'] = value.get('max')\n",
    "    # Flatten velocity data\n",
    "    velocity_data = data.get('velocity')\n",
    "    if isinstance(velocity_data, dict):\n",
    "        for key, value in velocity_data.items():\n",
    "            data[f'velocity_{key}_min'] = value.get('min')\n",
    "            data[f'velocity_{key}_mean'] = value.get('mean')\n",
    "            data[f'velocity_{key}_max'] = value.get('max')\n",
    "\n",
    "    # Flatten coordinates data\n",
    "    coordinates_data = data.get('coordinates')\n",
    "    if isinstance(coordinates_data, dict):\n",
    "        for key, value in coordinates_data.items():\n",
    "            if key in ['start', 'end']:\n",
    "                for sub_key, sub_value in value.items():\n",
    "                    data[f'coordinates_{key}_{sub_key}'] = sub_value\n",
    "\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once done, instantiate a pandas DataFrame by calling `pd.DataFrame()`, passing `data_list` as its parameter. Store the result to `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now check the dataset stored in Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unnecessary columns `velocity`, `coordinates`, and `acceleration` as these are already flattened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropColumns = ['velocity', 'coordinates', 'acceleration']\n",
    "df.drop(columns = dropColumns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the dataset for the next stage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export the dataset, use `df.to_csv()` to convert the dataframe into a CSV file, where the paramaters are:\n",
    "\n",
    "- *path_or_buf*: `'collected-haptic-dataset.csv'` (the filename)\n",
    "- *index*: `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"collected-haptic-dataset.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
