{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install scipy\n",
    "%pip install numpy\n",
    "%pip install seaborn\n",
    "%pip install matplotlib\n",
    "%pip install statsmodels\n",
    "%pip install scikit-learn\n",
    "%pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Machine Learning and Hyperparamater Tuning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions needed for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "def pretty_print(dictionary):\n",
    "    pp = pprint.PrettyPrinter(indent=1)\n",
    "    pp.pprint(dictionary)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_df(df):\n",
    "    # Print the DataFrame\n",
    "    with pd.option_context('display.max_rows', 20, 'display.max_columns', None): \n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "def pearson_correlation(y_true, y_pred):\n",
    "    return pearsonr(y_true, y_pred)[0]\n",
    "\n",
    "\n",
    "scorers = {\n",
    "    'neg_mean_absolute_error': 'neg_mean_absolute_error',\n",
    "    'neg_mean_absolute_percentage_error': 'neg_mean_absolute_percentage_error',\n",
    "    'neg_median_absolute_error': 'neg_median_absolute_error',\n",
    "    'pearson_corr': make_scorer(pearson_correlation)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "def feature_importance(explainer, X_test):\n",
    "\n",
    "    shap_values = explainer(X_test)\n",
    "\n",
    "    # Summarize the feature importances\n",
    "    shap_summary = np.abs(shap_values.values).mean(axis=0)\n",
    "\n",
    "    # Get the most contributing feature\n",
    "    most_contributing_feature = X_test.columns[np.argmax(shap_summary)]\n",
    "\n",
    "    print(f\"The most contributing feature is: {most_contributing_feature}\")\n",
    "\n",
    "    # Calculate the mean absolute SHAP values for each feature\n",
    "    mean_abs_shap_values = np.abs(shap_values.values).mean(axis=0)\n",
    "    feature_names = shap_values.feature_names\n",
    "\n",
    "    # Sort the SHAP values and feature names in descending order\n",
    "    sorted_indices = np.argsort(mean_abs_shap_values)[::-1]\n",
    "    sorted_shap_values = mean_abs_shap_values[sorted_indices]\n",
    "    sorted_feature_names = np.array(feature_names)[sorted_indices]\n",
    "\n",
    "    # Create a horizontal bar plot using Matplotlib\n",
    "    fig, ax = plt.subplots()\n",
    "    bars = ax.barh(sorted_feature_names, sorted_shap_values, color='orange')\n",
    "    ax.set_xlabel(\"Mean |SHAP value| (impact on model output)\")\n",
    "    ax.set_title(\"SHAP Feature Importance\")\n",
    "\n",
    "    # Adding text labels for each bar\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height() / 2, f'{width:.4f}', ha='left', va='center')\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    shap.plots.beeswarm(shap_values)\n",
    "    shap.plots.scatter(shap_values[:, most_contributing_feature])\n",
    "\n",
    "    print(\"Force Plot on First Sample\")\n",
    "    shap.plots.force(shap_values[0], matplotlib=True)\n",
    "\n",
    "    print(\"Force Plot on Last Sample\")\n",
    "    shap.plots.force(shap_values[-1], matplotlib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import median_absolute_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def linr_ml(X, y, X_test, y_test):\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'fit_intercept':[True, False],\n",
    "        'positive': [True, False]\n",
    "    }\n",
    "\n",
    "    # Initialize the Random Forest regressor\n",
    "    rf_regressor = LinearRegression()\n",
    "\n",
    "    # Perform Grid Search Cross-Validation\n",
    "    grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=10, scoring=scorers, refit='neg_mean_absolute_error', n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Evaluate the best model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    performances = {\n",
    "        'mean_absolute_error': mean_absolute_error(y_test, y_pred),\n",
    "        'median_absolute_error': median_absolute_error(y_test, y_pred),\n",
    "        'mean_absolute_percentage_error': mean_absolute_percentage_error(y_test, y_pred),\n",
    "        'pearson_corr': pearsonr(y_test, y_pred)[0]\n",
    "    }\n",
    "\n",
    "    # feature_importance(shap.LinearExplainer(best_model, X), X_test)\n",
    "    return best_model, performances, best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additive Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import median_absolute_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def addr_ml(X,y,X_test, y_test):\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [10, 50, 100],\n",
    "        'max_depth': [10, 20],\n",
    "        'min_samples_split': [5, 10],\n",
    "        'min_samples_leaf': [2, 4],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'learning_rate': [0.001, 0.01, 0.1]\n",
    "    }\n",
    "\n",
    "    # Initialize the Random Forest regressor\n",
    "    rf_regressor = GradientBoostingRegressor(random_state=42, loss='absolute_error')\n",
    "\n",
    "    # Perform Grid Search Cross-Validation\n",
    "    grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=10, scoring=scorers, refit='neg_mean_absolute_error', n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Evaluate the best model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    performances = {\n",
    "        'mean_absolute_error': mean_absolute_error(y_test, y_pred),\n",
    "        'median_absolute_error': median_absolute_error(y_test, y_pred),\n",
    "        'mean_absolute_percentage_error': mean_absolute_percentage_error(y_test, y_pred),\n",
    "        'pearson_corr': pearsonr(y_test, y_pred)[0]\n",
    "    }\n",
    "\n",
    "    # explainer = shap.TreeExplainer(best_rf_model)\n",
    "    # feature_importance(explainer, X_test)\n",
    "\n",
    "\n",
    "\n",
    "    return best_model, performances, best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import median_absolute_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def dest_ml(X,y,X_test, y_test):\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'criterion': ['friedman_mse', 'absolute_error'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': [10, 20, 30, 50],\n",
    "        'min_samples_split': [5, 10, 20, 30],\n",
    "        'min_samples_leaf': [2, 4, 8, 10],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "\n",
    "    # Initialize the Random Forest regressor\n",
    "    rf_regressor = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "    # Perform Grid Search Cross-Validation\n",
    "    grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=10, scoring=scorers, refit='neg_mean_absolute_error', n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Evaluate the best model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    performances = {\n",
    "        'mean_absolute_error': mean_absolute_error(y_test, y_pred),\n",
    "        'median_absolute_error': median_absolute_error(y_test, y_pred),\n",
    "        'mean_absolute_percentage_error': mean_absolute_percentage_error(y_test, y_pred),\n",
    "        'pearson_corr': pearsonr(y_test, y_pred)[0]\n",
    "    }\n",
    "\n",
    "    # explainer = shap.TreeExplainer(best_rf_model)\n",
    "    # feature_importance(explainer, X_test)\n",
    "\n",
    "    return best_model, performances, best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import median_absolute_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "def knn_ml(X,y,X_test, y_test):\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['ball_tree', 'kd_tree', 'auto'],\n",
    "        'leaf_size': [30, 40, 50],\n",
    "        'p': [1, 2],\n",
    "        'metric': ['minkowski', 'l1', 'l2']\n",
    "    }\n",
    "\n",
    "    # Initialize the Random Forest regressor\n",
    "    rf_regressor = KNeighborsRegressor()\n",
    "\n",
    "    # Perform Grid Search Cross-Validation\n",
    "    grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=10, scoring=scorers, refit='neg_mean_absolute_error', n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Evaluate the best model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    performances = {\n",
    "        'mean_absolute_error': mean_absolute_error(y_test, y_pred),\n",
    "        'median_absolute_error': median_absolute_error(y_test, y_pred),\n",
    "        'mean_absolute_percentage_error': mean_absolute_percentage_error(y_test, y_pred),\n",
    "        'pearson_corr': pearsonr(y_test, y_pred)[0]\n",
    "    }\n",
    "\n",
    "    # explainer = shap.KernelExplainer(best_rf_model.predict, shap.kmeans(X, 5))\n",
    "    # feature_importance(explainer, X_test)\n",
    "\n",
    "    return best_model, performances, best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import median_absolute_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def rf_ml(X,y,X_test, y_test):\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [10, 50, 100, 200],\n",
    "        'max_depth': [10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Initialize the Random Forest regressor\n",
    "    rf_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "    # Perform Grid Search Cross-Validation\n",
    "    grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=10, scoring=scorers, refit='neg_mean_absolute_error', n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Evaluate the best model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    performances = {\n",
    "        'mean_absolute_error': mean_absolute_error(y_test, y_pred),\n",
    "        'median_absolute_error': median_absolute_error(y_test, y_pred),\n",
    "        'mean_absolute_percentage_error': mean_absolute_percentage_error(y_test, y_pred),\n",
    "        'pearson_corr': pearsonr(y_test, y_pred)[0]\n",
    "    }\n",
    "\n",
    "    # explainer = shap.TreeExplainer(best_rf_model)\n",
    "    # feature_importance(explainer, X_test)\n",
    "\n",
    "    return best_model, performances, best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (Linear Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.metrics import median_absolute_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def lin_svr_ml(X,y,X_test, y_test):\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'epsilon': [0.01, 0.1, 0.5],\n",
    "    'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'], \n",
    "    'intercept_scaling': [10, 50, 75],  \n",
    "    'max_iter': [3000, 4000, 5000]\n",
    "    }\n",
    "\n",
    "    # Initialize the Random Forest regressor\n",
    "    regressor = LinearSVR(random_state=42)\n",
    "\n",
    "    # Perform Grid Search Cross-Validation\n",
    "    grid_search = GridSearchCV(estimator=regressor, param_grid=param_grid, cv=10, scoring=scorers, refit='neg_mean_absolute_error', n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Evaluate the best model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    performances = {\n",
    "        'mean_absolute_error': mean_absolute_error(y_test, y_pred),\n",
    "        'median_absolute_error': median_absolute_error(y_test, y_pred),\n",
    "        'mean_absolute_percentage_error': mean_absolute_percentage_error(y_test, y_pred),\n",
    "        'pearson_corr': pearsonr(y_test, y_pred)[0]\n",
    "    }\n",
    "\n",
    "    # explainer = shap.KernelExplainer(best_model.predict, shap.sample(X, 5))\n",
    "    # feature_importance(explainer, X_test)\n",
    "\n",
    "    return best_model, performances, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (RBF-Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import median_absolute_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def rbf_svr_ml(X,y,X_test, y_test):\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'epsilon': [0.01, 0.1, 0.5],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10], \n",
    "    'degree': [2, 3, 4],  \n",
    "    'shrinking': [True, False]\n",
    "    }\n",
    "\n",
    "    # Initialize the Random Forest regressor\n",
    "    regressor = SVR(kernel='rbf')\n",
    "\n",
    "    # Perform Grid Search Cross-Validation\n",
    "    grid_search = GridSearchCV(estimator=regressor, param_grid=param_grid, cv=10, scoring=scorers, refit='neg_mean_absolute_error', n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Evaluate the best model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    performances = {\n",
    "        'mean_absolute_error': mean_absolute_error(y_test, y_pred),\n",
    "        'median_absolute_error': median_absolute_error(y_test, y_pred),\n",
    "        'mean_absolute_percentage_error': mean_absolute_percentage_error(y_test, y_pred),\n",
    "        'pearson_corr': pearsonr(y_test, y_pred)[0]\n",
    "    }\n",
    "\n",
    "    # explainer = shap.KernelExplainer(best_model.predict, shap.sample(X, 5))\n",
    "    # feature_importance(explainer, X_test)\n",
    "\n",
    "    \n",
    "    return best_model, performances, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"whole.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Valence\n",
    "features_valence = np.genfromtxt('whole_df_val_sig.txt', dtype=str)\n",
    "X_valence = df[features_valence]\n",
    "print_df(X_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Arousal\n",
    "features_arousal = np.genfromtxt('whole_df_ars_sig.txt', dtype=str)\n",
    "X_arousal = df[features_arousal]\n",
    "print_df(X_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Y Labels\n",
    "y = df[['valenceValue', 'arousalValue']]\n",
    "print_df(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_valence, X_test_valence, y_train_valence, y_test_valence = train_test_split(X_valence, y['valenceValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Valence Training data shape:', X_train_valence.shape)\n",
    "print('Valence Test data shape:', X_test_valence.shape)\n",
    "print('Training labels shape (Valence):', y_train_valence.shape)\n",
    "print('Test labels shape (Valence):', y_test_valence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train_arousal, X_test_arousal, y_train_arousal, y_test_arousal = train_test_split(X_arousal, y['arousalValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Arousal Training data shape:', X_train_arousal.shape)\n",
    "print('Arousal Test data shape:', X_test_arousal.shape)\n",
    "print('Training labels shape (Arousal):', y_train_arousal.shape)\n",
    "print('Test labels shape (Arousal):', y_test_arousal.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_valence), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_arousal), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additive Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (Linear Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = lin_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (RBF-Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rbf_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolated Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"whole-extrapolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Valence\n",
    "features_valence = np.genfromtxt('whole_ex_val_sig.txt', dtype=str)\n",
    "X_valence = df[features_valence]\n",
    "print_df(X_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Arousal\n",
    "features_arousal = np.genfromtxt('whole_ex_ars_sig.txt', dtype=str)\n",
    "X_arousal = df[features_arousal]\n",
    "print_df(X_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Y Labels\n",
    "y = df[['valenceValue', 'arousalValue']]\n",
    "print_df(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_valence, X_test_valence, y_train_valence, y_test_valence = train_test_split(X_valence, y['valenceValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Valence Training data shape:', X_train_valence.shape)\n",
    "print('Valence Test data shape:', X_test_valence.shape)\n",
    "print('Training labels shape (Valence):', y_train_valence.shape)\n",
    "print('Test labels shape (Valence):', y_test_valence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train_arousal, X_test_arousal, y_train_arousal, y_test_arousal = train_test_split(X_arousal, y['arousalValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Arousal Training data shape:', X_train_arousal.shape)\n",
    "print('Arousal Test data shape:', X_test_arousal.shape)\n",
    "print('Training labels shape (Arousal):', y_train_arousal.shape)\n",
    "print('Test labels shape (Arousal):', y_test_arousal.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_valence), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_arousal), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additive Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (Linear Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = lin_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (RBF-Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rbf_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadrant 1 (High Valence - High Arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"q1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Valence\n",
    "features_valence = np.genfromtxt('q1_df_val_sig.txt', dtype=str)\n",
    "X_valence = df[features_valence]\n",
    "print_df(X_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Arousal\n",
    "features_arousal = np.genfromtxt('q1_df_ars_sig.txt', dtype=str)\n",
    "X_arousal = df[features_arousal]\n",
    "print_df(X_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Y Labels\n",
    "y = df[['valenceValue', 'arousalValue']]\n",
    "print_df(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_valence, X_test_valence, y_train_valence, y_test_valence = train_test_split(X_valence, y['valenceValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Valence Training data shape:', X_train_valence.shape)\n",
    "print('Valence Test data shape:', X_test_valence.shape)\n",
    "print('Training labels shape (Valence):', y_train_valence.shape)\n",
    "print('Test labels shape (Valence):', y_test_valence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train_arousal, X_test_arousal, y_train_arousal, y_test_arousal = train_test_split(X_arousal, y['arousalValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Arousal Training data shape:', X_train_arousal.shape)\n",
    "print('Arousal Test data shape:', X_test_arousal.shape)\n",
    "print('Training labels shape (Arousal):', y_train_arousal.shape)\n",
    "print('Test labels shape (Arousal):', y_test_arousal.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_valence), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_arousal), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additive Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (Linear Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = lin_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (RBF-Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rbf_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolated Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"q1-extrapolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Valence\n",
    "features_valence = np.genfromtxt('q1_ex_val_sig.txt', dtype=str)\n",
    "X_valence = df[features_valence]\n",
    "print_df(X_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Arousal\n",
    "features_arousal = np.genfromtxt('q1_ex_ars_sig.txt', dtype=str)\n",
    "X_arousal = df[features_arousal]\n",
    "print_df(X_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Y Labels\n",
    "y = df[['valenceValue', 'arousalValue']]\n",
    "print_df(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_valence, X_test_valence, y_train_valence, y_test_valence = train_test_split(X_valence, y['valenceValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Valence Training data shape:', X_train_valence.shape)\n",
    "print('Valence Test data shape:', X_test_valence.shape)\n",
    "print('Training labels shape (Valence):', y_train_valence.shape)\n",
    "print('Test labels shape (Valence):', y_test_valence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train_arousal, X_test_arousal, y_train_arousal, y_test_arousal = train_test_split(X_arousal, y['arousalValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Arousal Training data shape:', X_train_arousal.shape)\n",
    "print('Arousal Test data shape:', X_test_arousal.shape)\n",
    "print('Training labels shape (Arousal):', y_train_arousal.shape)\n",
    "print('Test labels shape (Arousal):', y_test_arousal.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_valence), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_arousal), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additive Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (Linear Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = lin_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (RBF-Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rbf_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadrant 2 (Low Valence - High Arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"q2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Valence\n",
    "features_valence = np.genfromtxt('q2_df_val_sig.txt', dtype=str)\n",
    "X_valence = df[features_valence]\n",
    "print_df(X_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Arousal\n",
    "features_arousal = np.genfromtxt('q2_df_ars_sig.txt', dtype=str)\n",
    "X_arousal = df[features_arousal]\n",
    "print_df(X_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Y Labels\n",
    "y = df[['valenceValue', 'arousalValue']]\n",
    "print_df(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_valence, X_test_valence, y_train_valence, y_test_valence = train_test_split(X_valence, y['valenceValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Valence Training data shape:', X_train_valence.shape)\n",
    "print('Valence Test data shape:', X_test_valence.shape)\n",
    "print('Training labels shape (Valence):', y_train_valence.shape)\n",
    "print('Test labels shape (Valence):', y_test_valence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train_arousal, X_test_arousal, y_train_arousal, y_test_arousal = train_test_split(X_arousal, y['arousalValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Arousal Training data shape:', X_train_arousal.shape)\n",
    "print('Arousal Test data shape:', X_test_arousal.shape)\n",
    "print('Training labels shape (Arousal):', y_train_arousal.shape)\n",
    "print('Test labels shape (Arousal):', y_test_arousal.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_valence), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_arousal), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additive Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (Linear Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = lin_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (RBF-Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rbf_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolated Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"q2-extrapolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Valence\n",
    "features_valence = np.genfromtxt('q2_ex_val_sig.txt', dtype=str)\n",
    "X_valence = df[features_valence]\n",
    "print_df(X_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Arousal\n",
    "features_arousal = np.genfromtxt('q2_ex_ars_sig.txt', dtype=str)\n",
    "X_arousal = df[features_arousal]\n",
    "print_df(X_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Y Labels\n",
    "y = df[['valenceValue', 'arousalValue']]\n",
    "print_df(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_valence, X_test_valence, y_train_valence, y_test_valence = train_test_split(X_valence, y['valenceValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Valence Training data shape:', X_train_valence.shape)\n",
    "print('Valence Test data shape:', X_test_valence.shape)\n",
    "print('Training labels shape (Valence):', y_train_valence.shape)\n",
    "print('Test labels shape (Valence):', y_test_valence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train_arousal, X_test_arousal, y_train_arousal, y_test_arousal = train_test_split(X_arousal, y['arousalValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Arousal Training data shape:', X_train_arousal.shape)\n",
    "print('Arousal Test data shape:', X_test_arousal.shape)\n",
    "print('Training labels shape (Arousal):', y_train_arousal.shape)\n",
    "print('Test labels shape (Arousal):', y_test_arousal.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_valence), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_arousal), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additive Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (Linear Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = lin_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (RBF-Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rbf_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadrant 3 (Low Valence - Low Arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"q3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Valence\n",
    "features_valence = np.genfromtxt('q3_df_val_sig.txt', dtype=str)\n",
    "X_valence = df[features_valence]\n",
    "print_df(X_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Arousal\n",
    "features_arousal = np.genfromtxt('q3_df_ars_sig.txt', dtype=str)\n",
    "X_arousal = df[features_arousal]\n",
    "print_df(X_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Y Labels\n",
    "y = df[['valenceValue', 'arousalValue']]\n",
    "print_df(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_valence, X_test_valence, y_train_valence, y_test_valence = train_test_split(X_valence, y['valenceValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Valence Training data shape:', X_train_valence.shape)\n",
    "print('Valence Test data shape:', X_test_valence.shape)\n",
    "print('Training labels shape (Valence):', y_train_valence.shape)\n",
    "print('Test labels shape (Valence):', y_test_valence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train_arousal, X_test_arousal, y_train_arousal, y_test_arousal = train_test_split(X_arousal, y['arousalValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Arousal Training data shape:', X_train_arousal.shape)\n",
    "print('Arousal Test data shape:', X_test_arousal.shape)\n",
    "print('Training labels shape (Arousal):', y_train_arousal.shape)\n",
    "print('Test labels shape (Arousal):', y_test_arousal.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_valence), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_arousal), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additive Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (Linear Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = lin_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (RBF-Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rbf_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolated Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"q3-extrapolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Valence\n",
    "features_valence = np.genfromtxt('q3_ex_val_sig.txt', dtype=str)\n",
    "X_valence = df[features_valence]\n",
    "print_df(X_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Arousal\n",
    "features_arousal = np.genfromtxt('q3_ex_ars_sig.txt', dtype=str)\n",
    "X_arousal = df[features_arousal]\n",
    "print_df(X_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Y Labels\n",
    "y = df[['valenceValue', 'arousalValue']]\n",
    "print_df(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_valence, X_test_valence, y_train_valence, y_test_valence = train_test_split(X_valence, y['valenceValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Valence Training data shape:', X_train_valence.shape)\n",
    "print('Valence Test data shape:', X_test_valence.shape)\n",
    "print('Training labels shape (Valence):', y_train_valence.shape)\n",
    "print('Test labels shape (Valence):', y_test_valence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train_arousal, X_test_arousal, y_train_arousal, y_test_arousal = train_test_split(X_arousal, y['arousalValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Arousal Training data shape:', X_train_arousal.shape)\n",
    "print('Arousal Test data shape:', X_test_arousal.shape)\n",
    "print('Training labels shape (Arousal):', y_train_arousal.shape)\n",
    "print('Test labels shape (Arousal):', y_test_arousal.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_valence), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_arousal), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additive Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (Linear Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = lin_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (RBF-Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rbf_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadrant 4 (High Valence - Low Arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"q4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Valence\n",
    "features_valence = np.genfromtxt('q4_df_val_sig.txt', dtype=str)\n",
    "X_valence = df[features_valence]\n",
    "print_df(X_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Arousal\n",
    "features_arousal = np.genfromtxt('q4_df_ars_sig.txt', dtype=str)\n",
    "X_arousal = df[features_arousal]\n",
    "print_df(X_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Y Labels\n",
    "y = df[['valenceValue', 'arousalValue']]\n",
    "print_df(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_valence, X_test_valence, y_train_valence, y_test_valence = train_test_split(X_valence, y['valenceValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Valence Training data shape:', X_train_valence.shape)\n",
    "print('Valence Test data shape:', X_test_valence.shape)\n",
    "print('Training labels shape (Valence):', y_train_valence.shape)\n",
    "print('Test labels shape (Valence):', y_test_valence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train_arousal, X_test_arousal, y_train_arousal, y_test_arousal = train_test_split(X_arousal, y['arousalValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Arousal Training data shape:', X_train_arousal.shape)\n",
    "print('Arousal Test data shape:', X_test_arousal.shape)\n",
    "print('Training labels shape (Arousal):', y_train_arousal.shape)\n",
    "print('Test labels shape (Arousal):', y_test_arousal.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_valence), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_arousal), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additive Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (Linear Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = lin_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (RBF-Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rbf_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolated Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"q4-extrapolated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Valence\n",
    "features_valence = np.genfromtxt('q4_ex_val_sig.txt', dtype=str)\n",
    "X_valence = df[features_valence]\n",
    "print_df(X_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X Data for Arousal\n",
    "features_arousal = np.genfromtxt('q4_ex_ars_sig.txt', dtype=str)\n",
    "X_arousal = df[features_arousal]\n",
    "print_df(X_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Y Labels\n",
    "y = df[['valenceValue', 'arousalValue']]\n",
    "print_df(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_valence, X_test_valence, y_train_valence, y_test_valence = train_test_split(X_valence, y['valenceValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Valence Training data shape:', X_train_valence.shape)\n",
    "print('Valence Test data shape:', X_test_valence.shape)\n",
    "print('Training labels shape (Valence):', y_train_valence.shape)\n",
    "print('Test labels shape (Valence):', y_test_valence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train_arousal, X_test_arousal, y_train_arousal, y_test_arousal = train_test_split(X_arousal, y['arousalValue'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('Arousal Training data shape:', X_train_arousal.shape)\n",
    "print('Arousal Test data shape:', X_test_arousal.shape)\n",
    "print('Training labels shape (Arousal):', y_train_arousal.shape)\n",
    "print('Test labels shape (Arousal):', y_test_arousal.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_valence), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = linr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.LinearExplainer(model, X_train_arousal), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additive Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = addr_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = dest_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = knn_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.kmeans(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(shap.TreeExplainer(model), X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (Linear Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = lin_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (RBF-Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rbf_svr_ml(X_train_valence, y_train_valence, X_test_valence, y_test_valence)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_valence, 5))\n",
    "feature_importance(explainer, X_test_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, performances, best_params = rf_ml(X_train_arousal, y_train_arousal, X_test_arousal, y_test_arousal)\n",
    "\n",
    "print(\"\\n Paramaters of Best Model: \")\n",
    "pretty_print(best_params)\n",
    "\n",
    "print(\"\\n Model Performances: \")\n",
    "pretty_print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_arousal, 5))\n",
    "feature_importance(explainer, X_test_arousal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
